{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and directories\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DATA_DIR = '../2021VRDL_HW1_datasets'\n",
    "CKPT_DIR = '../checkpoints/resnext101_32x8d'\n",
    "RLS_CKPT_DIR = '../checkpoints/RELEASE'\n",
    "OUT_DIR = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLDR_HW1_Dataset(Dataset):\n",
    "    def __init__(self, labels_file_dir, img_dir, transform=None, target_transform=None):\n",
    "        with open(labels_file_dir) as f:\n",
    "            labels_file = list(f)\n",
    "\n",
    "        imgs = []\n",
    "        for row in labels_file:\n",
    "            row.strip()\n",
    "            words = row.split()\n",
    "            imgs.append((words[0], int(words[1].split('.')[0]) - 1))\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.imgs[idx]\n",
    "        img_path = os.path.join(self.img_dir, image_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise(object):\n",
    "    def __init__(self, intensity=0.05):\n",
    "        self.intensity = intensity\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn_like(tensor) * self.intensity\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(intensity={self.intensity})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the classes\n",
    "with open(f'{DATA_DIR}/classes.txt') as f:\n",
    "    classes = [x.strip() for x in f.readlines()]\n",
    "\n",
    "def get_class_display(pred):\n",
    "    # Returns the class name according to the given label\n",
    "    return classes[pred]\n",
    "\n",
    "def validate(net, val_dataloader):\n",
    "    # Returns the accuracy of validation data\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "\n",
    "        correct_cnt = 0\n",
    "        all_cnt = 0\n",
    "\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            # Put inputs and labels to device\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Predict\n",
    "            outputs = net(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            correct_cnt += int(torch.sum(torch.eq(preds, labels)))\n",
    "            all_cnt += len(labels)\n",
    "\n",
    "        return correct_cnt / all_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the training data\n",
    "dataset = VLDR_HW1_Dataset(\n",
    "    f'{DATA_DIR}/training_labels.txt',\n",
    "    f'{DATA_DIR}/training_images/',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(degrees=(-30, 30)),\n",
    "        transforms.RandomPerspective(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        AddNoise()\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Split the data to train set and validation set\n",
    "train_size_ratio = 0.9\n",
    "train_size = int(len(dataset) * train_size_ratio)\n",
    "validate_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, validate_size]\n",
    ")\n",
    "\n",
    "# Dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training params\n",
    "from_epoch = 400\n",
    "to_epoch = 600\n",
    "save_per = 1\n",
    "learning_rate = 0.001  # 0.001\n",
    "momentum = 0.9  # 0.9\n",
    "\n",
    "# Initial net and set the size of final layer to 200\n",
    "net = torchvision.models.resnext101_32x8d(pretrained=True)\n",
    "num_features = net.fc.in_features\n",
    "net.fc = torch.nn.Linear(num_features, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if necessary\n",
    "if from_epoch > 0:\n",
    "    PATH = f'{CKPT_DIR}/{net.__class__.__name__}_{from_epoch}.pth'\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    print('Load checkpoint from', PATH)\n",
    "\n",
    "# Put net to device\n",
    "net = net.to(device)\n",
    "\n",
    "# Set criterion and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f'Start training from epoch {from_epoch} to {to_epoch} on device {device}')\n",
    "print(f'lr = {learning_rate}')\n",
    "\n",
    "for epoch in range(from_epoch, to_epoch):\n",
    "    running_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        net.train()\n",
    "\n",
    "        # Put inputs and labels to device\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and optimize\n",
    "        outputs = net(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch + 1} loss: {running_loss / len(train_dataloader)}')\n",
    "    running_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "    if (epoch + 1) % save_per == 0:\n",
    "        # Save the checkpoint and test\n",
    "        PATH = f'{CKPT_DIR}/{net.__class__.__name__}_{epoch + 1}.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        val_acc = validate(net, val_dataloader)\n",
    "        print(f'Epoch {epoch + 1} saved. Acc on validation data: {val_acc}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nets to predict testing data\n",
    "nets = [\n",
    "    (torchvision.models.resnet152(pretrained=False),\n",
    "        'resnet152_130e', 0.622156),\n",
    "    (torchvision.models.resnext101_32x8d(pretrained=False),\n",
    "        'resnext101_32x8d_advtrsf_160e', 0.679196),\n",
    "    (torchvision.models.resnext101_32x8d(pretrained=False),\n",
    "        'resnext101_32x8d_advtrsf_nonoised_250e', 0.661721),\n",
    "    (torchvision.models.resnext101_32x8d(pretrained=False),\n",
    "        'resnext101_32x8d_70e', 0.631058),\n",
    "    (torchvision.models.resnext101_32x8d(pretrained=False),\n",
    "        'resnext101_32x8d_400e', 0.666667),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial the nets\n",
    "for net, ckpt, _ in nets:\n",
    "    # Set the size of final layer to 200\n",
    "    num_features = net.fc.in_features\n",
    "    net.fc = torch.nn.Linear(num_features, 200)\n",
    "    \n",
    "    # Load the pretrained weughts\n",
    "    PATH = f'{RLS_CKPT_DIR}/{ckpt}.pth'\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    print('Load checkpoint from', PATH)\n",
    "    \n",
    "    # Put net to device\n",
    "    net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "submission = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Data transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize([224, 224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    with open(f'{DATA_DIR}/testing_img_order.txt') as f:\n",
    "        # Read all the testing images\n",
    "        test_image_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    for img_name in test_image_names:\n",
    "        # image order is important to your result\n",
    "        img_path = os.path.join(f'{DATA_DIR}/testing_images/', img_name)\n",
    "        img = transform(Image.open(img_path).convert('RGB'))\n",
    "        img = img[None, :]\n",
    "        img = img.to(device)\n",
    "\n",
    "        outputs_list = torch.zeros(200).to(device)\n",
    "        for net, _, weight in nets:\n",
    "            net.eval()\n",
    "            outputs = net(img)  # the predicted category\n",
    "            outputs_list = outputs_list + outputs * weight\n",
    "\n",
    "        _, predicted_class = torch.max(outputs_list, 1)\n",
    "        predicted_class_display = get_class_display(int(predicted_class))\n",
    "        submission.append([img_name, predicted_class_display])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission\n",
    "np.savetxt(f'{OUT_DIR}/answer.txt', submission, fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
